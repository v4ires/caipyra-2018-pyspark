{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Baseado em \"Introduction to Spark with Python, by Jose A. Dianes\"](https://github.com/jadianes/spark-py-notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até agora nós introduzimos a criação de RDD junto com algumas transformações básicas como `map` e` filter` e algumas ações como `count`,` take` e `collect`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook mostrará como amostrar RDDs. Em relação às transformações, o `sample` será introduzido, uma vez que será útil em muitos cenários de aprendizagem estatística. Então, vamos comparar os resultados com a ação `takeSample`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazendo download do Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste notebook, usaremos o conjunto de dados reduzido (10 por cento) fornecido para a KDD Cup 1999, contendo quase meio milhão de interações de rede. O arquivo é fornecido como um arquivo *Gzip* que será baixado localmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "f = urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\", \"kddcup.data.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtendo os dados e criando o RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste caso, usaremos o conjunto de dados completo fornecido para a KDD Cup 1999, contendo quase meio milhão de interações de rede. O arquivo é fornecido como um arquivo Gzip que será baixado localmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos usar esse arquivo para criar nosso RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"./kddcup.data.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling RDDs   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Spark, há duas operações de amostragem, a transformação `sample` e a ação` takeSample`. Ao usar uma transformação, podemos dizer ao Spark para aplicar uma transformação sucessiva em uma amostra de um determinado RDD. Ao usar uma ação, recuperamos uma amostra e podemos armazená-la na memória local para ser usada por qualquer outra biblioteca padrão (por exemplo, Scikit-learn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A transformação `sample`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformação `sample` possui até três parâmetros. A primeira é se a amostragem é feita com substituição ou não. Em segundo lugar, o tamanho da amostra é uma fração. Por fim, podemos opcionalmente fornecer um *random seed*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size is 489957 of 4898431\n"
     ]
    }
   ],
   "source": [
    "raw_data_sample = raw_data.sample(False, 0.1, 1234)\n",
    "sample_size = raw_data_sample.count()\n",
    "total_size = raw_data.count()\n",
    "print(\"Sample size is {} of {}\".format(sample_size, total_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas o poder da amostragem como uma transformação vem de fazê-lo como parte de uma sequência de transformações adicionais. Isso será mais poderoso quando começarmos a fazer operações de agregações e pares de valores-chave e será especialmente útil ao usar a biblioteca de aprendizado de máquina do Spark MLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enquanto isso, imagine que queremos ter uma aproximação da proporção de interações `normal` em nosso conjunto de dados. Poderíamos fazer isso contando o número total de tags como fizemos nos cadernos anteriores. No entanto, queremos uma resposta mais rápida e não precisamos da resposta exata, mas apenas uma aproximação. Nós podemos fazer isso da seguinte maneira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of 'normal' interactions is 0.199\n",
      "Count done in 26.617 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "# transformations to be applied\n",
    "raw_data_sample_items = raw_data_sample.map(lambda x: x.split(\",\"))\n",
    "sample_normal_tags = raw_data_sample_items.filter(lambda x: \"normal.\" in x)\n",
    "\n",
    "# actions + time\n",
    "t0 = time()\n",
    "sample_normal_tags_count = sample_normal_tags.count()\n",
    "tt = time() - t0\n",
    "\n",
    "sample_normal_ratio = sample_normal_tags_count / float(sample_size)\n",
    "print(\"The ratio of 'normal' interactions is {}\".format(round(sample_normal_ratio,3)))\n",
    "print(\"Count done in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos comparar isso com o cálculo da proporção sem amostragem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of 'normal' interactions is 0.199\n",
      "Count done in 44.457 seconds\n"
     ]
    }
   ],
   "source": [
    "# transformations to be applied\n",
    "raw_data_items = raw_data.map(lambda x: x.split(\",\"))\n",
    "normal_tags = raw_data_items.filter(lambda x: \"normal.\" in x)\n",
    "\n",
    "# actions + time\n",
    "t0 = time()\n",
    "normal_tags_count = normal_tags.count()\n",
    "tt = time() - t0\n",
    "\n",
    "normal_ratio = normal_tags_count / float(total_size)\n",
    "print(\"The ratio of 'normal' interactions is {}\".format(round(normal_ratio,3)))\n",
    "print(\"Count done in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nós podemos ver um ganho no tempo. Quanto mais transformações aplicarmos após a amostragem, maior será esse ganho. Isso porque, sem amostrar, todas as transformações são aplicadas ao conjunto completo de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A ação `takeSample`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se o que precisamos é pegar uma amostra de dados brutos do nosso RDD na memória local para ser usado por outras bibliotecas não-Spark, o `takeSample` pode ser usado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sintaxe é muito semelhante, mas neste caso especificamos o número de itens em vez do tamanho da amostra como uma fração do tamanho completo dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of 'normal' interactions is 0.1988025\n",
      "Count done in 48.52 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "raw_data_sample = raw_data.takeSample(False, 400000, 1234)\n",
    "normal_data_sample = [x.split(\",\") for x in raw_data_sample if \"normal.\" in x]\n",
    "tt = time() - t0\n",
    "\n",
    "normal_sample_size = len(normal_data_sample)\n",
    "\n",
    "normal_ratio = normal_sample_size / 400000.0\n",
    "print(\"The ratio of 'normal' interactions is {}\".format(normal_ratio))\n",
    "print(\"Count done in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O processo foi muito parecido como antes. Obtivemos uma amostra de cerca de 10% dos dados e depois filtramos e dividimos.\n",
    "\n",
    "No entanto, demorou mais tempo, mesmo com uma amostra ligeiramente menor. O motivo é que o Spark acabou de distribuir a execução do processo de amostragem. A filtragem e divisão dos resultados foram feitas localmente em um único nó."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
