{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operações Básicas RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Baseado em \"Introduction to Spark with Python, by Jose A. Dianes\"](https://github.com/jadianes/spark-py-notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazendo download do Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste notebook, usaremos o conjunto de dados reduzido 10% do fornecido para a KDD Cup 1999, contendo quase meio milhão de interações de rede. O arquivo é fornecido como um arquivo *Gzip* que será baixado localmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "f = urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos usar esse arquivo para criar nosso RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A tranformação `filter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa transformação pode ser aplicada aos RDDs para manter apenas os elementos que satisfazem uma determinada condição. Mais concretamente, uma função é avaliada em cada elemento no RDD original. O novo RDD resultante conterá apenas os elementos que fazem a função retornar `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por exemplo, imagine que queremos contar quantas interações `normal` que temos em nosso conjunto de dados. Podemos filtrar nosso RDD `raw_data` da seguinte maneira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_raw_data = raw_data.filter(lambda x: 'normal.' in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos contar quantos elementos temos no novo RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 97278 'normal' interactions\n",
      "Count completed in 1.7 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "normal_count = normal_raw_data.count()\n",
    "tt = time() - t0\n",
    "print(\"There are {} 'normal' interactions\".format(normal_count))\n",
    "print(\"Count completed in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre-se do bloco de notas 1 que temos um total de 494021 em nosso conjunto de dados de 10%. Aqui podemos ver que 97278 contém a palavra `normal`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que medimos o tempo decorrido para contar os elementos no RDD. Fizemos isso porque queríamos apontar que computações reais (distribuídas) no Spark acontecem quando executamos *actions* e não *transformations*. Neste caso, `count` é a ação que executamos no RDD. Podemos aplicar quantas transformações quisermos em um nosso RDD e nenhuma computação ocorrerá até chamarmos a primeira ação que, nesse caso, leva alguns segundos para ser concluída."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A transformação `map`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando a transformação `map` no Spark, podemos aplicar uma função a todos os elementos do nosso RDD. Os lambdas do Python são especialmente expressivos para isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste caso, queremos ler nosso arquivo de dados como um arquivo formatado em CSV. Podemos fazer isso aplicando uma função lambda a cada elemento no RDD da seguinte maneira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse completed in 0.053 seconds\n",
      "['0',\n",
      " 'tcp',\n",
      " 'http',\n",
      " 'SF',\n",
      " '181',\n",
      " '5450',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '1',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '8',\n",
      " '8',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '1.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '9',\n",
      " '9',\n",
      " '1.00',\n",
      " '0.00',\n",
      " '0.11',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " 'normal.']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "t0 = time()\n",
    "head_rows = csv_data.take(5)\n",
    "tt = time() - t0\n",
    "print(\"Parse completed in {} seconds\".format(round(tt,3)))\n",
    "pprint(head_rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais uma vez, toda a ação acontece quando chamamos a primeira *action* do Spark, neste caso a operação *take*. E se pegarmos muitos elementos em vez de apenas os primeiros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse completed in 1.508 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "head_rows = csv_data.take(100000)\n",
    "tt = time() - t0\n",
    "print(\"Parse completed in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que demora mais tempo. A função `map` é aplicada agora de maneira distribuída a muitos elementos no RDD, portanto, o maior tempo de execução."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando `map` e funções predefinidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claro que podemos usar funções pré-definidas com o `map`. Imagine que queremos ter cada elemento no RDD como um par de valores-chave em que a chave é a tag (por exemplo, *normal*) e o valor é toda a lista de elementos que representa a linha no arquivo formatado em CSV. Nós poderíamos proceder da seguinte forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('normal.',\n",
      " ['0',\n",
      "  'tcp',\n",
      "  'http',\n",
      "  'SF',\n",
      "  '181',\n",
      "  '5450',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '1',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '8',\n",
      "  '8',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '1.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '9',\n",
      "  '9',\n",
      "  '1.00',\n",
      "  '0.00',\n",
      "  '0.11',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  'normal.'])\n"
     ]
    }
   ],
   "source": [
    "def parse_interaction(line):\n",
    "    elems = line.split(\",\")\n",
    "    tag = elems[41]\n",
    "    return (tag, elems)\n",
    "\n",
    "key_csv_data = raw_data.map(parse_interaction)\n",
    "head_rows = key_csv_data.take(5)\n",
    "pprint(head_rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso foi fácil, não foi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em nosso notebook sobre como trabalhar com pares de valores-chave, usaremos esse tipo de RDDs para fazer agregações de dados (por exemplo, contagem por chave)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A ação `collect`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até agora usamos as ações `count` e` take`. Outra ação básica que precisamos aprender é `collect`. Basicamente, ele irá colocar todos os elementos do RDD na memória para que possamos trabalhar com eles. Por esse motivo, ele deve ser usado com cuidado, especialmente quando se trabalha com RDDs grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um exemplo usando nossos dados brutos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected in 12.975 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "all_raw_data = raw_data.collect()\n",
    "tt = time() - t0\n",
    "print(\"Data collected in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso levou mais tempo como qualquer outra ação que usamos antes, é claro. Cada nó do trabalhador do Spark que possui um fragmento do RDD precisa ser coordenado para recuperar sua parte e, em seguida, agrupar todos os dados em uma única memória."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como último exemplo que combina todos os anteriores, queremos coletar todas as interações `normal` como pares de chave-valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected in 11.298 seconds\n",
      "There are 97278 'normal' interactions\n"
     ]
    }
   ],
   "source": [
    "# get data from file\n",
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "\n",
    "# parse into key-value pairs\n",
    "key_csv_data = raw_data.map(parse_interaction)\n",
    "\n",
    "# filter normal key interactions\n",
    "normal_key_interactions = key_csv_data.filter(lambda x: x[0] == \"normal.\")\n",
    "\n",
    "# collect all\n",
    "t0 = time()\n",
    "all_normal = normal_key_interactions.collect()\n",
    "tt = time() - t0\n",
    "normal_count = len(all_normal)\n",
    "\n",
    "print(\"Data collected in {} seconds\".format(round(tt,3)))\n",
    "print(\"There are {} 'normal' interactions\".format(normal_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
