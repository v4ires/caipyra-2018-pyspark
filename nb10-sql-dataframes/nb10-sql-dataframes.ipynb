{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL e Data Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Baseado em \"Introduction to Spark with Python, by Jose A. Dianes\"](https://github.com/jadianes/spark-py-notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook apresentará os recursos do Spark para lidar com dados de maneira estruturada. Basicamente, tudo gira em torno do conceito de *Data Frame* e usando *SQL language* para consultá-los. Veremos como a abstração do quadro de dados, muito popular em outros ecossistemas de análise de dados (por exemplo, R e Python/Pandas), é muito poderosa ao executar a análise exploratória de dados. De fato, é muito fácil expressar consultas de dados quando usadas em conjunto com a linguagem SQL. Além disso, o Spark distribui essa estrutura de dados baseada em colunas de forma transparente, a fim de tornar o processo de consulta o mais eficiente possível."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazendo download do Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste notebook, usaremos o conjunto de dados reduzido (10 por cento) fornecido para a KDD Cup 1999, contendo quase meio milhão de interações de rede. O arquivo é fornecido como um arquivo *Gzip* que será baixado localmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "f = urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtendo os dados e criando o RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como fizemos nos cadernos anteriores, usaremos o conjunto de dados reduzido (10%) fornecido para a [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html), contendo quase a metade milhões de interações nework. O arquivo é fornecido como um arquivo Gzip que será baixado localmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtendo um quadro de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um Spark `DataFrame` é uma coleção distribuída de dados organizados em colunas nomeadas. É conceitualmente equivalente a uma tabela em um banco de dados relacional ou a um quadro de dados em R ou Pandas. Eles podem ser construídos a partir de uma ampla variedade de fontes, como um RDD existente no nosso caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O ponto de entrada em toda a funcionalidade do SQL no Spark é a classe `SQLContext`. Para criar uma instância básica, tudo o que precisamos é de uma referência ao `SparkContext`. Como estamos executando o Spark no modo shell (usando pySpark), podemos usar o objeto de contexto global `sc` para essa finalidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferir o esquema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com um `SQLContext`, estamos prontos para criar um `DataFrame` a partir do nosso RDD existente. Mas primeiro precisamos informar ao Spark SQL o esquema em nossos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Spark SQL pode converter um RDD de objetos `Row` em um` DataFrame`. As linhas são construídas passando uma lista de pares chave/valor como *kwargs* para a classe `Row`. As chaves definem os nomes das colunas e os tipos são inferidos observando a primeira linha. Portanto, é importante que não haja dados ausentes na primeira linha do RDD para inferir corretamente o esquema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No nosso caso, primeiro precisamos dividir os dados separados por vírgula e, em seguida, usar as informações na descrição da tarefa do KDD de 1999 para obter os [column names](http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "csv_data = raw_data.map(lambda l: l.split(\",\"))\n",
    "row_data = csv_data.map(lambda p: Row(\n",
    "    duration=int(p[0]), \n",
    "    protocol_type=p[1],\n",
    "    service=p[2],\n",
    "    flag=p[3],\n",
    "    src_bytes=int(p[4]),\n",
    "    dst_bytes=int(p[5])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que tenhamos o nosso RDD de `Row`, podemos inferir e registrar o esquema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = sqlContext.createDataFrame(row_data)\n",
    "interactions_df.registerTempTable(\"interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos executar consultas SQL sobre nosso quadro de dados que foi registrado como uma tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|duration|dst_bytes|\n",
      "+--------+---------+\n",
      "|    5057|        0|\n",
      "|    5059|        0|\n",
      "|    5051|        0|\n",
      "|    5056|        0|\n",
      "|    5051|        0|\n",
      "|    5039|        0|\n",
      "|    5062|        0|\n",
      "|    5041|        0|\n",
      "|    5056|        0|\n",
      "|    5064|        0|\n",
      "|    5043|        0|\n",
      "|    5061|        0|\n",
      "|    5049|        0|\n",
      "|    5061|        0|\n",
      "|    5048|        0|\n",
      "|    5047|        0|\n",
      "|    5044|        0|\n",
      "|    5063|        0|\n",
      "|    5068|        0|\n",
      "|    5062|        0|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select tcp network interactions with more than 1 second duration and no transfer from destination\n",
    "tcp_interactions = sqlContext.sql(\"\"\"\n",
    "    SELECT duration, dst_bytes FROM interactions WHERE protocol_type = 'tcp' AND duration > 1000 AND dst_bytes = 0\n",
    "\"\"\")\n",
    "tcp_interactions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os resultados das consultas SQL são RDDs e suportam todas as operações RDD normais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 5057, Dest. bytes: 0\n",
      "Duration: 5059, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5056, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5039, Dest. bytes: 0\n",
      "Duration: 5062, Dest. bytes: 0\n",
      "Duration: 5041, Dest. bytes: 0\n",
      "Duration: 5056, Dest. bytes: 0\n",
      "Duration: 5064, Dest. bytes: 0\n",
      "Duration: 5043, Dest. bytes: 0\n",
      "Duration: 5061, Dest. bytes: 0\n",
      "Duration: 5049, Dest. bytes: 0\n",
      "Duration: 5061, Dest. bytes: 0\n",
      "Duration: 5048, Dest. bytes: 0\n",
      "Duration: 5047, Dest. bytes: 0\n",
      "Duration: 5044, Dest. bytes: 0\n",
      "Duration: 5063, Dest. bytes: 0\n",
      "Duration: 5068, Dest. bytes: 0\n",
      "Duration: 5062, Dest. bytes: 0\n",
      "Duration: 5046, Dest. bytes: 0\n",
      "Duration: 5052, Dest. bytes: 0\n",
      "Duration: 5044, Dest. bytes: 0\n",
      "Duration: 5054, Dest. bytes: 0\n",
      "Duration: 5039, Dest. bytes: 0\n",
      "Duration: 5058, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5032, Dest. bytes: 0\n",
      "Duration: 5063, Dest. bytes: 0\n",
      "Duration: 5040, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5066, Dest. bytes: 0\n",
      "Duration: 5044, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5036, Dest. bytes: 0\n",
      "Duration: 5055, Dest. bytes: 0\n",
      "Duration: 2426, Dest. bytes: 0\n",
      "Duration: 5047, Dest. bytes: 0\n",
      "Duration: 5057, Dest. bytes: 0\n",
      "Duration: 5037, Dest. bytes: 0\n",
      "Duration: 5057, Dest. bytes: 0\n",
      "Duration: 5062, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5053, Dest. bytes: 0\n",
      "Duration: 5064, Dest. bytes: 0\n",
      "Duration: 5044, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5033, Dest. bytes: 0\n",
      "Duration: 5066, Dest. bytes: 0\n",
      "Duration: 5063, Dest. bytes: 0\n",
      "Duration: 5056, Dest. bytes: 0\n",
      "Duration: 5042, Dest. bytes: 0\n",
      "Duration: 5063, Dest. bytes: 0\n",
      "Duration: 5060, Dest. bytes: 0\n",
      "Duration: 5056, Dest. bytes: 0\n",
      "Duration: 5049, Dest. bytes: 0\n",
      "Duration: 5043, Dest. bytes: 0\n",
      "Duration: 5039, Dest. bytes: 0\n",
      "Duration: 5041, Dest. bytes: 0\n",
      "Duration: 42448, Dest. bytes: 0\n",
      "Duration: 42088, Dest. bytes: 0\n",
      "Duration: 41065, Dest. bytes: 0\n",
      "Duration: 40929, Dest. bytes: 0\n",
      "Duration: 40806, Dest. bytes: 0\n",
      "Duration: 40682, Dest. bytes: 0\n",
      "Duration: 40571, Dest. bytes: 0\n",
      "Duration: 40448, Dest. bytes: 0\n",
      "Duration: 40339, Dest. bytes: 0\n",
      "Duration: 40232, Dest. bytes: 0\n",
      "Duration: 40121, Dest. bytes: 0\n",
      "Duration: 36783, Dest. bytes: 0\n",
      "Duration: 36674, Dest. bytes: 0\n",
      "Duration: 36570, Dest. bytes: 0\n",
      "Duration: 36467, Dest. bytes: 0\n",
      "Duration: 36323, Dest. bytes: 0\n",
      "Duration: 36204, Dest. bytes: 0\n",
      "Duration: 32038, Dest. bytes: 0\n",
      "Duration: 31925, Dest. bytes: 0\n",
      "Duration: 31809, Dest. bytes: 0\n",
      "Duration: 31709, Dest. bytes: 0\n",
      "Duration: 31601, Dest. bytes: 0\n",
      "Duration: 31501, Dest. bytes: 0\n",
      "Duration: 31401, Dest. bytes: 0\n",
      "Duration: 31301, Dest. bytes: 0\n",
      "Duration: 31194, Dest. bytes: 0\n",
      "Duration: 31061, Dest. bytes: 0\n",
      "Duration: 30935, Dest. bytes: 0\n",
      "Duration: 30835, Dest. bytes: 0\n",
      "Duration: 30735, Dest. bytes: 0\n",
      "Duration: 30619, Dest. bytes: 0\n",
      "Duration: 30518, Dest. bytes: 0\n",
      "Duration: 30418, Dest. bytes: 0\n",
      "Duration: 30317, Dest. bytes: 0\n",
      "Duration: 30217, Dest. bytes: 0\n",
      "Duration: 30077, Dest. bytes: 0\n",
      "Duration: 25420, Dest. bytes: 0\n",
      "Duration: 22921, Dest. bytes: 0\n",
      "Duration: 22821, Dest. bytes: 0\n",
      "Duration: 22721, Dest. bytes: 0\n",
      "Duration: 22616, Dest. bytes: 0\n",
      "Duration: 22516, Dest. bytes: 0\n",
      "Duration: 22416, Dest. bytes: 0\n",
      "Duration: 22316, Dest. bytes: 0\n",
      "Duration: 22216, Dest. bytes: 0\n",
      "Duration: 21987, Dest. bytes: 0\n",
      "Duration: 21887, Dest. bytes: 0\n",
      "Duration: 21767, Dest. bytes: 0\n",
      "Duration: 21661, Dest. bytes: 0\n",
      "Duration: 21561, Dest. bytes: 0\n",
      "Duration: 21455, Dest. bytes: 0\n",
      "Duration: 21334, Dest. bytes: 0\n",
      "Duration: 21223, Dest. bytes: 0\n",
      "Duration: 21123, Dest. bytes: 0\n",
      "Duration: 20983, Dest. bytes: 0\n",
      "Duration: 14682, Dest. bytes: 0\n",
      "Duration: 14420, Dest. bytes: 0\n",
      "Duration: 14319, Dest. bytes: 0\n",
      "Duration: 14198, Dest. bytes: 0\n",
      "Duration: 14098, Dest. bytes: 0\n",
      "Duration: 13998, Dest. bytes: 0\n",
      "Duration: 13898, Dest. bytes: 0\n",
      "Duration: 13796, Dest. bytes: 0\n",
      "Duration: 13678, Dest. bytes: 0\n",
      "Duration: 13578, Dest. bytes: 0\n",
      "Duration: 13448, Dest. bytes: 0\n",
      "Duration: 13348, Dest. bytes: 0\n",
      "Duration: 13241, Dest. bytes: 0\n",
      "Duration: 13141, Dest. bytes: 0\n",
      "Duration: 13033, Dest. bytes: 0\n",
      "Duration: 12933, Dest. bytes: 0\n",
      "Duration: 12833, Dest. bytes: 0\n",
      "Duration: 12733, Dest. bytes: 0\n",
      "Duration: 12001, Dest. bytes: 0\n",
      "Duration: 5678, Dest. bytes: 0\n",
      "Duration: 5010, Dest. bytes: 0\n",
      "Duration: 1298, Dest. bytes: 0\n",
      "Duration: 1031, Dest. bytes: 0\n",
      "Duration: 36438, Dest. bytes: 0\n"
     ]
    }
   ],
   "source": [
    "# Output duration together with dst_bytes\n",
    "tcp_interactions_out = tcp_interactions.rdd.map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes))\n",
    "for ti_out in tcp_interactions_out.collect():\n",
    "  print(ti_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos facilmente dar uma olhada no nosso esquema de quadros de dados usando o `printSchema`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dst_bytes: long (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- protocol_type: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- src_bytes: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consultas como operações `DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Spark `DataFrame` fornece uma linguagem específica de domínio para manipulação de dados estruturados. Essa linguagem inclui métodos que podemos concatenar para fazer seleção, filtragem, agrupamento, etc. Por exemplo, digamos que queremos contar quantas interações existem para cada tipo de protocolo. Nós podemos proceder da seguinte forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|protocol_type| count|\n",
      "+-------------+------+\n",
      "|          tcp|190065|\n",
      "|          udp| 20354|\n",
      "|         icmp|283602|\n",
      "+-------------+------+\n",
      "\n",
      "Query performed in 11.984 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "interactions_df.select(\"protocol_type\", \"duration\", \"dst_bytes\").groupBy(\"protocol_type\").count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print(\"Query performed in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora imagine que queremos contar quantas interações duram mais de 1 segundo, sem transferência de dados do destino, agrupadas por tipo de protocolo. Podemos apenas adicionar para filtrar chamadas para o anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|protocol_type|count|\n",
      "+-------------+-----+\n",
      "|          tcp|  139|\n",
      "+-------------+-----+\n",
      "\n",
      "Query performed in 14.18 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "interactions_df.select(\"protocol_type\", \"duration\", \"dst_bytes\").filter(interactions_df.duration>1000).filter(interactions_df.dst_bytes==0).groupBy(\"protocol_type\").count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print(\"Query performed in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar isso para realizar algumas [exploratory data analysis](http://en.wikipedia.org/wiki/Exploratory_data_analysis). Vamos contar quantas interações normais e de ataque nós temos. Primeiro, precisamos adicionar a coluna de rótulo aos nossos dados.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_type(label):\n",
    "    if label!=\"normal.\":\n",
    "        return \"attack\"\n",
    "    else:\n",
    "        return \"normal\"\n",
    "    \n",
    "row_labeled_data = csv_data.map(lambda p: Row(\n",
    "    duration=int(p[0]), \n",
    "    protocol_type=p[1],\n",
    "    service=p[2],\n",
    "    flag=p[3],\n",
    "    src_bytes=int(p[4]),\n",
    "    dst_bytes=int(p[5]),\n",
    "    label=get_label_type(p[41])\n",
    "    )\n",
    ")\n",
    "interactions_labeled_df = sqlContext.createDataFrame(row_labeled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desta vez, não precisamos registrar o esquema, pois vamos usar a interface de consulta OO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos verificar o anterior realmente funciona contando dados de ataque e normal em nosso quadro de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| label| count|\n",
      "+------+------+\n",
      "|normal| 97278|\n",
      "|attack|396743|\n",
      "+------+------+\n",
      "\n",
      "Query performed in 9.763 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "interactions_labeled_df.select(\"label\").groupBy(\"label\").count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print(\"Query performed in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora queremos contá-las por rótulo e tipo de protocolo, para ver o quão importante é o tipo de protocolo detectar quando uma interação é ou não um ataque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------+\n",
      "| label|protocol_type| count|\n",
      "+------+-------------+------+\n",
      "|normal|          udp| 19177|\n",
      "|normal|         icmp|  1288|\n",
      "|normal|          tcp| 76813|\n",
      "|attack|         icmp|282314|\n",
      "|attack|          tcp|113252|\n",
      "|attack|          udp|  1177|\n",
      "+------+-------------+------+\n",
      "\n",
      "Query performed in 8.901 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "interactions_labeled_df.select(\"label\", \"protocol_type\").groupBy(\"label\", \"protocol_type\").count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print(\"Query performed in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À primeira vista, parece que as interações *udp* estão em menor proporção entre ataques de rede e outros tipos de protocolos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E podemos fazer agrupamentos muito mais sofisticados. Por exemplo, adicione ao anterior um \"split\" baseado na transferência de dados do destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---------------+------+\n",
      "| label|protocol_type|(dst_bytes = 0)| count|\n",
      "+------+-------------+---------------+------+\n",
      "|normal|          udp|          false| 15583|\n",
      "|attack|          udp|          false|    11|\n",
      "|attack|          tcp|           true|110583|\n",
      "|normal|          tcp|          false| 67500|\n",
      "|attack|         icmp|           true|282314|\n",
      "|attack|          tcp|          false|  2669|\n",
      "|normal|          tcp|           true|  9313|\n",
      "|normal|          udp|           true|  3594|\n",
      "|normal|         icmp|           true|  1288|\n",
      "|attack|          udp|           true|  1166|\n",
      "+------+-------------+---------------+------+\n",
      "\n",
      "Query performed in 10.764 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "interactions_labeled_df.select(\"label\", \"protocol_type\", \"dst_bytes\").groupBy(\"label\", \"protocol_type\", interactions_labeled_df.dst_bytes==0).count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print(\"Query performed in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos quão relevante é essa nova divisão para determinar se uma interação de rede é um ataque."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos parar por aqui, mas podemos ver como esse tipo de consulta é poderosa para explorar nossos dados. Na verdade, podemos replicar todas as divisões que vimos nos cadernos anteriores, ao introduzir árvores de classificação, apenas selecionando, tateando e filtrando nosso dataframe. Para uma lista mais detalhada (mas menos real) das operações e fontes de dados do DataFrame do Spark, dê uma olhada na documentação oficial [aqui](https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframe-operations). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
